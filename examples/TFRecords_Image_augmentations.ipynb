{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-11T10:44:46.631365Z",
     "start_time": "2019-09-11T10:44:43.227423Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.16.2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "import tensorflow\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import gc\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "jobs = int(os.cpu_count()/2)\n",
    "\n",
    "print(np.__version__)\n",
    "import time, math\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "import copy\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import os\n",
    "from importlib import reload\n",
    "\n",
    "import pandas as pd\n",
    "import fastnet as fn\n",
    "from fastnet.model.utils import ConvBn2D\n",
    "from fastnet.misc import msg\n",
    "\n",
    "from tensorflow.keras.backend import clear_session\n",
    "\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(\"App\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-11T10:44:48.569768Z",
     "start_time": "2019-09-11T10:44:48.565167Z"
    }
   },
   "outputs": [],
   "source": [
    "random_pad_crop = fn.get_first_argument_transformer(fn.get_random_pad_crop(4,4,32,32))\n",
    "cutout = fn.get_first_argument_transformer(fn.get_cutout_eraser(0,255))\n",
    "hflip = fn.get_first_argument_transformer(fn.get_hflip_aug())\n",
    "\n",
    "transformations = fn.combine_transformers(random_pad_crop,hflip,cutout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Sub-Classed Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-11T10:27:52.077635Z",
     "start_time": "2019-09-11T10:27:44.032587Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = BATCH_SIZE = 512\n",
    "EPOCHS = 5\n",
    "\n",
    "train,test = fn.get_cifar10(\"cifar10\",batch_size)\n",
    "\n",
    "train = train.map(transformations).map(lambda x,y: (x/255.0,y)).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "test = test.map(lambda x,y: (x/255.0,y)).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-11T07:07:31.851420Z",
     "start_time": "2019-09-11T07:07:31.841047Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class SimpleCifarNet(tf.keras.Model):\n",
    "    def __init__(self, start_kernels=32, weight=0.125, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        c = start_kernels\n",
    "        self.max_pool = tf.keras.layers.MaxPooling2D()\n",
    "        self.init_conv_bn = ConvBn2D(c, kernel_size=3)\n",
    "        self.c0 = ConvBn2D(c, kernel_size=3)\n",
    "        \n",
    "        self.c1 = ConvBn2D(c*2, kernel_size=3)\n",
    "        self.c2 = ConvBn2D(c*2, kernel_size=3)\n",
    "        \n",
    "        self.c3 = ConvBn2D(c*4, kernel_size=3)\n",
    "        self.c4 = ConvBn2D(c*4, kernel_size=3)\n",
    "        \n",
    "\n",
    "        self.pool = tf.keras.layers.GlobalMaxPool2D()\n",
    "        self.linear = tf.keras.layers.Dense(10, kernel_initializer='glorot_uniform', use_bias=False)\n",
    "        self.weight = weight\n",
    "        \n",
    "    def call(self, x):\n",
    "        \n",
    "        h = self.max_pool(self.c0(self.init_conv_bn(x)))\n",
    "        h = self.max_pool(self.c2(self.c1(h)))\n",
    "        h = self.max_pool(self.c4(self.c3(h)))\n",
    "        h = self.pool(h)\n",
    "        h = self.linear(h) * self.weight\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-11T07:14:45.566638Z",
     "start_time": "2019-09-11T07:07:36.900322Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "98/98 [==============================] - 87s 884ms/step - loss: 1.8586 - sparse_categorical_accuracy: 0.2502 - sparse_top_k_categorical_accuracy: 0.5665\n",
      "Epoch 2/5\n",
      "98/98 [==============================] - 86s 873ms/step - loss: 1.5127 - sparse_categorical_accuracy: 0.4209 - sparse_top_k_categorical_accuracy: 0.7726\n",
      "Epoch 3/5\n",
      "98/98 [==============================] - 84s 860ms/step - loss: 1.3675 - sparse_categorical_accuracy: 0.4917 - sparse_top_k_categorical_accuracy: 0.8138\n",
      "Epoch 4/5\n",
      "98/98 [==============================] - 85s 868ms/step - loss: 1.2692 - sparse_categorical_accuracy: 0.5310 - sparse_top_k_categorical_accuracy: 0.8423\n",
      "Epoch 5/5\n",
      "98/98 [==============================] - 87s 887ms/step - loss: 1.1845 - sparse_categorical_accuracy: 0.5652 - sparse_top_k_categorical_accuracy: 0.8593\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xb359c00b8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SimpleCifarNet(start_kernels=8,dynamic=True)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(0.01),\n",
    "              loss=loss,metrics=[tf.keras.metrics.SparseCategoricalAccuracy(),tf.keras.metrics.SparseTopKCategoricalAccuracy(k=3)])\n",
    "\n",
    "callbacks=[]\n",
    "\n",
    "model.fit(train, epochs=EPOCHS, callbacks=callbacks,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-11T07:30:09.206939Z",
     "start_time": "2019-09-11T07:30:05.583568Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     20/Unknown - 4s 180ms/step - loss: 1.7010 - sparse_categorical_accuracy: 0.4326 - sparse_top_k_categorical_accuracy: 0.7531\n",
      "Loss = 1.70, Acc = 0.43, Top 3 Acc = 0.75\n"
     ]
    }
   ],
   "source": [
    "res = model.evaluate(test)\n",
    "print(\"\\nLoss = %.2f, Acc = %.2f, Top 3 Acc = %.2f\"%tuple(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-11T07:02:45.926126Z",
     "start_time": "2019-09-11T07:02:45.503879Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[512 32 32 3] [512 1]\n",
      "tf.float32 tf.int32\n"
     ]
    }
   ],
   "source": [
    "train,test = fn.get_cifar10(\"cifar10\",batch_size)\n",
    "\n",
    "train = train.map(transformations).map(lambda x,y: (x/255.0,y))\n",
    "test = test.map(lambda x,y: (x/255.0,y))\n",
    "for x,y in train.take(1):\n",
    "    tf.print(tf.shape(x),tf.shape(y))\n",
    "    tf.print(x.dtype,y.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Training Loop with One Cycle LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-11T10:45:11.925693Z",
     "start_time": "2019-09-11T10:45:11.914531Z"
    }
   },
   "outputs": [],
   "source": [
    "class SimpleCifarLoopNet(tf.keras.Model):\n",
    "    def __init__(self, start_kernels=32, weight=0.125, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        c = start_kernels\n",
    "        self.max_pool = tf.keras.layers.MaxPooling2D()\n",
    "        self.init_conv_bn = ConvBn2D(c, kernel_size=3)\n",
    "        self.c0 = ConvBn2D(c, kernel_size=3)\n",
    "        \n",
    "        self.c1 = ConvBn2D(c*2, kernel_size=3)\n",
    "        self.c2 = ConvBn2D(c*2, kernel_size=3)\n",
    "        \n",
    "        self.c3 = ConvBn2D(c*4, kernel_size=3)\n",
    "        self.c4 = ConvBn2D(c*4, kernel_size=3)\n",
    "        \n",
    "\n",
    "        self.pool = tf.keras.layers.GlobalMaxPool2D()\n",
    "        self.linear = tf.keras.layers.Dense(10, kernel_initializer='glorot_uniform', use_bias=False)\n",
    "        self.weight = weight\n",
    "        \n",
    "    def call(self, x, y):\n",
    "        \n",
    "        h = self.max_pool(self.c0(self.init_conv_bn(x)))\n",
    "        h = self.max_pool(self.c2(self.c1(h)))\n",
    "        h = self.max_pool(self.c4(self.c3(h)))\n",
    "        h = self.pool(h)\n",
    "        h = self.linear(h) * self.weight\n",
    "        \n",
    "        y = tf.reshape(y,shape=[tf.shape(y)[0]])\n",
    "        ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=h, labels=y)\n",
    "        loss = tf.reduce_sum(ce)\n",
    "        correct = tf.reduce_sum(tf.cast(tf.math.equal(tf.argmax(h, axis = 1), y), tf.float32))\n",
    "        return loss, correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-11T10:53:17.443199Z",
     "start_time": "2019-09-11T10:45:12.544398Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0911 16:16:56.061398 140735639765888 <ipython-input-4-c21831f5303e>:52]  epoch =  0 ||train=> loss: 1.991 acc: 0.265 ||val=> loss: 2.259 val acc: 0.215 103.5s\n",
      "I0911 16:18:26.425060 140735639765888 <ipython-input-4-c21831f5303e>:52]  epoch =  1 ||train=> loss: 1.539 acc: 0.434 ||val=> loss: 1.679 val acc: 0.416 193.8s\n",
      "I0911 16:20:09.369811 140735639765888 <ipython-input-4-c21831f5303e>:52]  epoch =  2 ||train=> loss: 1.289 acc: 0.532 ||val=> loss: 1.099 val acc: 0.600 296.8s\n",
      "I0911 16:21:45.334099 140735639765888 <ipython-input-4-c21831f5303e>:52]  epoch =  3 ||train=> loss: 1.183 acc: 0.574 ||val=> loss: 1.052 val acc: 0.620 392.7s\n",
      "I0911 16:23:17.439698 140735639765888 <ipython-input-4-c21831f5303e>:52]  epoch =  4 ||train=> loss: 1.136 acc: 0.589 ||val=> loss: 1.008 val acc: 0.636 484.8s\n",
      "I0911 16:23:17.440636 140735639765888 <ipython-input-4-c21831f5303e>:57]  Train acc =  0.5894 Test acc = 0.6365 Time Taken =  484.83531522750854\n"
     ]
    }
   ],
   "source": [
    "batch_size = BATCH_SIZE = 512\n",
    "EPOCHS = 5\n",
    "len_train = 50000\n",
    "len_test = 10000\n",
    "batches_per_epoch = len_train//BATCH_SIZE + 1\n",
    "WEIGHT_DECAY = 1e-4\n",
    "mid_epoch = 2\n",
    "pre_end_epoch = 3\n",
    "\n",
    "\n",
    "global_step = {\"batch\":0}\n",
    "momentum_schedule =  lambda t: np.interp([t], [0, mid_epoch, pre_end_epoch, EPOCHS], [0.9, 0.8, 0.9, 0.9])[0] \n",
    "momentum_func = lambda: momentum_schedule(global_step[\"batch\"]/batches_per_epoch)\n",
    "\n",
    "lr_schedule = lambda t: np.interp([t], [0, mid_epoch, pre_end_epoch, EPOCHS], [0.04, 0.4, 0.04, 0.01])[0] # LR = 0.75\n",
    "lr_func = lambda: lr_schedule(global_step[\"batch\"]/batches_per_epoch)/BATCH_SIZE\n",
    "\n",
    "opt = tf.keras.optimizers.SGD(learning_rate=lr_func, momentum=momentum_func, nesterov=True)\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "\n",
    "model = SimpleCifarLoopNet(start_kernels=8,dynamic=True)\n",
    "\n",
    "t = time.time()\n",
    "for epoch in range(EPOCHS):\n",
    "    tf.keras.backend.set_learning_phase(1)    \n",
    "    train_loss = test_loss = train_acc = test_acc = 0.0\n",
    "    train,test = fn.get_cifar10(\"cifar10\",batch_size)\n",
    "    train = train.map(transformations).map(lambda x,y: (x/255.0,y)).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    test = test.map(lambda x,y: (x/255.0,y)).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    for (x, y) in train:\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            loss, correct = model(x, y)\n",
    "\n",
    "        var = model.trainable_variables\n",
    "        grads = tape.gradient(loss, var)\n",
    "        for g, v in zip(grads, var):\n",
    "            g += v * WEIGHT_DECAY * BATCH_SIZE\n",
    "        _ = opt.apply_gradients(zip(grads, var))\n",
    "        global_step[\"batch\"] = global_step[\"batch\"]+1\n",
    "        train_loss += loss.numpy()\n",
    "        train_acc += correct.numpy()\n",
    "        train_accs.append(train_acc / len_train)\n",
    "        \n",
    "    tf.keras.backend.set_learning_phase(0)\n",
    "    for (x, y) in test:\n",
    "        loss, correct = model(x, y)\n",
    "        test_loss += loss.numpy()\n",
    "        test_acc += correct.numpy()\n",
    "    test_accs.append(test_acc / len_test)\n",
    "    logger.debug(msg(\"epoch = %2s\"%epoch,'||train=> loss: %.3f' %(train_loss / len_train), 'acc: %.3f' % (train_acc / len_train), '||val=> loss: %.3f' % (test_loss / len_test), 'val acc: %.3f' %(test_acc / len_test), '%.1fs'%(time.time() - t)))\n",
    "    \n",
    "    \n",
    "\n",
    "time_spent = time.time() - t\n",
    "logger.debug(msg(\"Train acc = \",train_accs[-1], \"Test acc =\",test_accs[-1], \"Time Taken = \", time_spent))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Images and Results (Coming Soon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Particular Class misclassified as Particular class\n",
    "# Particular Class misclassified as Any class\n",
    "# Any Class misclassified as Particular class\n",
    "# Any Class misclassified as Any class\n",
    "# Confusion Matrix and Common misclassifications\n",
    "\n",
    "# Top Confusions of Particular Actual Class\n",
    "# Top Confusions of Particular Predicted Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
