{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow Data module exposes a fast API for building data pipelines to feed ML models. To do this we convert our data to `tf.data.Dataset`, the easiest way is to use `tf.data.Dataset.from_tensor_slices(tuple_of_numpy_arrays)`.\n",
    "While this API is easy to use, it has the demerit of converting to from Numpy array to tensors all the time.\n",
    "\n",
    "To alleviate this issue we use TF-Records and store the data in `.tfrecord` files. While using TF-Record is useful, notice that using `tf.train.Example` is not the most performant method. As a result we store tensors directly. \n",
    "\n",
    "A second performance bottleneck arises when image augmentations are done one by one on images, not in batches. To alleviate this we provide few augmentations in batch versions so that augmentations can be fast.\n",
    "\n",
    "**We have 2 performance improvements:**\n",
    "1. Storing tensors as tfrecord instead of using `tf.train.Example`\n",
    "2. Adding batch image augmentations\n",
    "\n",
    "**Results (1 Epoch with 3 Augmentations)**:\n",
    "\n",
    "| **Method**                                | **Batch** | **Performance**     |\n",
    "|-------------------------------------------|-----------|---------------------|\n",
    "| `tf.data.Dataset.from_tensor_slices`      | False     | 37.3 s ± 1.42 s     |\n",
    "| `tf.data.Dataset.from_tensor_slices`      | True      | 9.83 s ± 569 ms     |\n",
    "| `tf.train.Example`                        | False     | 39.6 s ± 2.66 s     |\n",
    "| `tf.train.Example`                        | True      | 8.25 s ± 510 ms     |\n",
    "| Storing Tensors Directly                  | False     | 38.7 s ± 4.41 s     |\n",
    "| **Storing Tensors Directly (Our Method)** | **True**  | **7.68 s ± 763 ms** |\n",
    "\n",
    "**Machine Details**\n",
    "```\n",
    "MacBook Pro (Retina, 13-inch, Early 2015)\n",
    "\n",
    "  Model Name:\tMacBook Pro\n",
    "  Model Identifier:\tMacBookPro12,1\n",
    "  Processor Name:\tIntel Core i7\n",
    "  Processor Speed:\t3.1 GHz\n",
    "  Number of Processors:\t1\n",
    "  Total Number of Cores:\t2\n",
    "  L2 Cache (per Core):\t256 KB\n",
    "  L3 Cache:\t4 MB\n",
    "  Memory:\t16 GB\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-11T10:56:01.289498Z",
     "start_time": "2019-09-11T10:55:57.822428Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.16.2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "import tensorflow\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import gc\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "print(np.__version__)\n",
    "import time, math\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "import copy\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import os\n",
    "from importlib import reload\n",
    "\n",
    "import pandas as pd\n",
    "import fastnet as fn\n",
    "\n",
    "from tensorflow.keras.backend import clear_session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-11T10:56:01.724423Z",
     "start_time": "2019-09-11T10:56:01.719664Z"
    }
   },
   "outputs": [],
   "source": [
    "random_pad_crop = fn.get_first_argument_transformer(fn.get_random_pad_crop(4,4,32,32))\n",
    "cutout = fn.get_first_argument_transformer(fn.get_cutout_eraser(-1.0,1.0))\n",
    "hflip = fn.get_first_argument_transformer(fn.get_hflip_aug())\n",
    "\n",
    "transformations = fn.combine_transformers(random_pad_crop,hflip,cutout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-11T10:56:02.259823Z",
     "start_time": "2019-09-11T10:56:02.256137Z"
    }
   },
   "outputs": [],
   "source": [
    "jobs = int(os.cpu_count()/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark 1: Reading from Numpy \n",
    "reading with `tf.data.Dataset.from_tensor_slices(numpy_arrays)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T21:36:52.060586Z",
     "start_time": "2019-09-10T21:36:11.980123Z"
    }
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "batch_size = 1\n",
    "\n",
    "def mapper(x,y):\n",
    "    return tf.cast(x,tf.float32),y\n",
    "\n",
    "train = train.map(mapper).batch(batch_size)\n",
    "imgs = 50000\n",
    "batches = imgs//batch_size\n",
    "cf10_ex = []\n",
    "for x,y in train.map(transformations).take(batches):\n",
    "    cf10_ex.append(x.shape)\n",
    "\n",
    "# 37.3 s ± 1.42 s per loop (mean ± std. dev. of 7 runs, 1 loop each)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T21:39:35.237197Z",
     "start_time": "2019-09-10T21:38:17.150491Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "batch_size = 512\n",
    "\n",
    "def mapper(x,y):\n",
    "    return tf.cast(x,tf.float32),y\n",
    "\n",
    "train = train.map(mapper).batch(batch_size)\n",
    "imgs = 50000\n",
    "batches = imgs//batch_size\n",
    "cf10_ex = []\n",
    "for x,y in train.map(transformations).take(batches):\n",
    "    cf10_ex.append(x.shape)\n",
    "\n",
    "# 9.83 s ± 569 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark 2: TF-Example Batch vs No Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T21:45:14.514900Z",
     "start_time": "2019-09-10T21:39:49.871628Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39.6 s ± 2.66 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "batch_size = 1\n",
    "imgs = 50000\n",
    "batches = imgs//batch_size\n",
    "cf10_ex = []\n",
    "train,test = fn.get_cifar10_examples(\"cifar10\",batch_size)\n",
    "for x,y in train.map(transformations).take(batches):\n",
    "    cf10_ex.append(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-11T11:20:21.339926Z",
     "start_time": "2019-09-11T11:19:58.852435Z"
    }
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "batch_size = 512\n",
    "imgs = 50000\n",
    "batches = imgs//batch_size\n",
    "cf10_ex = []\n",
    "train,test = fn.get_cifar10_examples(\"cifar10\",batch_size)\n",
    "for x,y in train.map(transformations).take(batches):\n",
    "    cf10_ex.append(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark 3: Direct Tensor Storage (No Batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T22:01:28.672173Z",
     "start_time": "2019-09-10T21:56:19.727495Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.7 s ± 4.41 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "batch_size = 1\n",
    "imgs = 50000\n",
    "batches = imgs//batch_size\n",
    "cf10_ex = []\n",
    "train,test = fn.get_cifar10(\"cifar10\",batch_size)\n",
    "for x,y in train.map(transformations).take(batches):\n",
    "    cf10_ex.append(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark 4: Batch+Direct Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-11T11:17:42.505706Z",
     "start_time": "2019-09-11T11:16:28.105369Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.31 s ± 226 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "batch_size = 512\n",
    "imgs = 50000\n",
    "batches = imgs//batch_size\n",
    "cf10_ex = []\n",
    "train,test = fn.get_cifar10(\"cifar10\",batch_size)\n",
    "for x,y in train.map(transformations).take(batches):\n",
    "    cf10_ex.append(tf.shape(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T22:06:48.560878Z",
     "start_time": "2019-09-10T22:05:41.623704Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.2 s ± 856 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "batch_size = 512\n",
    "imgs = 50000\n",
    "batches = imgs//batch_size\n",
    "cf10_ex = []\n",
    "train,test = fn.get_cifar10(\"cifar10\",batch_size)\n",
    "for x,y in train.map(random_pad_crop).map(cutout).map(hflip).take(batches):\n",
    "    cf10_ex.append(tf.shape(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
